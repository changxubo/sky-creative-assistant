{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f5bfd57",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "from typing_extensions import TypedDict\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "\n",
    "class State(TypedDict):\n",
    "    \"State to manage all chats\"\n",
    "\n",
    "def custom_node(\n",
    "    state: State,\n",
    "):\n",
    "    \"Custom Node\"\n",
    "\n",
    "# Initialize the graph builder with a state\n",
    "builder = StateGraph(State)\n",
    "builder.add_node(\"supervisor\", custom_node)\n",
    "builder.add_node(\"planner\", custom_node)\n",
    "builder.add_node(\"reporter\", custom_node)\n",
    "builder.add_node(\"team_work\", custom_node)\n",
    "builder.add_node(\"researcher_agent\", custom_node)\n",
    "builder.add_node(\"developer_agent\", custom_node)\n",
    "builder.add_node(\"human_feedback\", custom_node)\n",
    "builder.add_node(\"podcast_agent\", custom_node)\n",
    "\n",
    "builder.add_edge(START, \"supervisor\")\n",
    "    \n",
    "builder.add_edge(\"supervisor\", \"planner\")\n",
    "builder.add_edge(\"supervisor\", END)\n",
    "    \n",
    "builder.add_edge(\"planner\", \"reporter\")\n",
    "builder.add_edge(\"planner\", \"human_feedback\")\n",
    "    \n",
    "builder.add_edge(\"human_feedback\", \"reporter\")\n",
    "builder.add_edge(\"human_feedback\", \"team_work\")\n",
    "builder.add_edge(\"human_feedback\", \"planner\")\n",
    "builder.add_edge(\"human_feedback\", END)\n",
    "    \n",
    "builder.add_edge(\"team_work\", \"planner\")\n",
    "builder.add_edge(\"team_work\", \"developer_agent\")\n",
    "builder.add_edge(\"team_work\", \"researcher_agent\")\n",
    "    \n",
    "builder.add_edge(\"reporter\", \"podcast_agent\")\n",
    "builder.add_edge(\"podcast_agent\", END)\n",
    "# Compile the graph\n",
    "workflow = builder.compile()\n",
    "\n",
    "Image(workflow.get_graph().draw_mermaid_png(padding=20),width=4096, height=4096)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0801edcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(\n",
    "  base_url = \"https://integrate.api.nvidia.com/v1\",\n",
    "  api_key = \"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\"\n",
    ")\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "  model=\"qwen/qwen3-235b-a22b\",\n",
    "  messages=[{\"role\":\"system\",\"content\":\"\"},{\"role\":\"user\",\"content\":\"Write a limerick about the wonders of GPU computing.\"}],\n",
    "  temperature=0.2,\n",
    "  top_p=0.7,\n",
    "  max_tokens=8192,\n",
    "  extra_body={\"chat_template_kwargs\": {\"thinking\":True}},\n",
    "  stream=True\n",
    ")\n",
    "\n",
    "for chunk in completion:\n",
    "  print(chunk)\n",
    "  reasoning = getattr(chunk.choices[0].delta, \"reasoning_content\", None)\n",
    "  if reasoning:\n",
    "    print(reasoning, end=\"\")\n",
    "  if chunk.choices[0].delta.content is not None:\n",
    "    print(chunk.choices[0].delta.content, end=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e05c6db6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "client = ChatOpenAI(\n",
    "  #base_url=\"https://integrate.api.nvidia.com/v1\",\n",
    "  #model=\"qwen/qwen3-235b-a22b\",\n",
    "  #api_key=\"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\", \n",
    "  base_url=\"https://dashscope.aliyuncs.com/compatible-mode/v1\",\n",
    "  model=\"qwen3-235b-a22b\",\n",
    "  api_key=\"sk-ba9202fabcb249e291c5907a86b13f80\",\n",
    "  temperature=0.6,\n",
    "  top_p=0.7,\n",
    "  max_tokens=8192,\n",
    "  #extra_body={\"chat_template_kwargs\": {\"thinking\":True}},\n",
    "  extra_body={\"enable_thinking\": True},\n",
    "  stream_usage=True, \n",
    ")\n",
    " \n",
    "response = client.stream(\n",
    "  input=[{\"role\":\"system\",\"content\":\"\"},\n",
    "   {\"role\":\"user\",\"content\":\"Write a limerick about the wonders of GPU computing.\"}\n",
    "   ])\n",
    "for chunk in response:\n",
    "    print(chunk)\n",
    "    print(chunk.text(), end=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "66ad2f17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='' additional_kwargs={} response_metadata={} id='run--1d220fee-cf9c-40a3-9bdc-a8bc8c868c64'\n",
      "content='' additional_kwargs={'reasoning_content': 'Okay'} response_metadata={} id='run--1d220fee-cf9c-40a3-9bdc-a8bc8c868c64'\n",
      "content='' additional_kwargs={'reasoning_content': ','} response_metadata={} id='run--1d220fee-cf9c-40a3-9bdc-a8bc8c868c64'\n",
      "content='' additional_kwargs={'reasoning_content': ' the'} response_metadata={} id='run--1d220fee-cf9c-40a3-9bdc-a8bc8c868c64'\n",
      "content='' additional_kwargs={'reasoning_content': ' user wants a l'} response_metadata={} id='run--1d220fee-cf9c-40a3-9bdc-a8bc8c868c64'\n",
      "content='' additional_kwargs={'reasoning_content': 'imerick about the'} response_metadata={} id='run--1d220fee-cf9c-40a3-9bdc-a8bc8c868c64'\n",
      "content='' additional_kwargs={'reasoning_content': ' wonders of GPU computing'} response_metadata={} id='run--1d220fee-cf9c-40a3-9bdc-a8bc8c868c64'\n",
      "content='' additional_kwargs={'reasoning_content': '. Let me start'} response_metadata={} id='run--1d220fee-cf9c-40a3-9bdc-a8bc8c868c64'\n",
      "content='' additional_kwargs={'reasoning_content': ' by recalling what a'} response_metadata={} id='run--1d220fee-cf9c-40a3-9bdc-a8bc8c868c64'\n",
      "content='' additional_kwargs={'reasoning_content': ' limerick is'} response_metadata={} id='run--1d220fee-cf9c-40a3-9bdc-a8bc8c868c64'\n",
      "content='' additional_kwargs={'reasoning_content': \". It's a\"} response_metadata={} id='run--1d220fee-cf9c-40a3-9bdc-a8bc8c868c64'\n",
      "content='' additional_kwargs={'reasoning_content': ' five-line poem with'} response_metadata={} id='run--1d220fee-cf9c-40a3-9bdc-a8bc8c868c64'\n",
      "content='' additional_kwargs={'reasoning_content': ' an AABBA'} response_metadata={} id='run--1d220fee-cf9c-40a3-9bdc-a8bc8c868c64'\n",
      "content='' additional_kwargs={'reasoning_content': ' rhyme scheme, usually'} response_metadata={} id='run--1d220fee-cf9c-40a3-9bdc-a8bc8c868c64'\n",
      "content='' additional_kwargs={'reasoning_content': ' humorous or whimsical'} response_metadata={} id='run--1d220fee-cf9c-40a3-9bdc-a8bc8c868c64'\n",
      "content='' additional_kwargs={'reasoning_content': '. The first,'} response_metadata={} id='run--1d220fee-cf9c-40a3-9bdc-a8bc8c868c64'\n",
      "content='' additional_kwargs={'reasoning_content': ' second, and fifth'} response_metadata={} id='run--1d220fee-cf9c-40a3-9bdc-a8bc8c868c64'\n",
      "content='' additional_kwargs={'reasoning_content': ' lines rhyme, and'} response_metadata={} id='run--1d220fee-cf9c-40a3-9bdc-a8bc8c868c64'\n",
      "content='' additional_kwargs={'reasoning_content': ' the third and fourth'} response_metadata={} id='run--1d220fee-cf9c-40a3-9bdc-a8bc8c868c64'\n",
      "content='' additional_kwargs={'reasoning_content': ' have a different rhyme'} response_metadata={} id='run--1d220fee-cf9c-40a3-9bdc-a8bc8c868c64'\n",
      "content='' additional_kwargs={'reasoning_content': '.\\n\\nSo, I'} response_metadata={} id='run--1d220fee-cf9c-40a3-9bdc-a8bc8c868c64'\n",
      "content='' additional_kwargs={'reasoning_content': ' need to focus on'} response_metadata={} id='run--1d220fee-cf9c-40a3-9bdc-a8bc8c868c64'\n",
      "content='' additional_kwargs={'reasoning_content': \" GPU computing's wonders\"} response_metadata={} id='run--1d220fee-cf9c-40a3-9bdc-a8bc8c868c64'\n",
      "content='' additional_kwargs={'reasoning_content': '. Key points about'} response_metadata={} id='run--1d220fee-cf9c-40a3-9bdc-a8bc8c868c64'\n",
      "content='' additional_kwargs={'reasoning_content': ' GPUs: parallel processing'} response_metadata={} id='run--1d220fee-cf9c-40a3-9bdc-a8bc8c868c64'\n",
      "content='' additional_kwargs={'reasoning_content': ', speed, handling'} response_metadata={} id='run--1d220fee-cf9c-40a3-9bdc-a8bc8c868c64'\n",
      "content='' additional_kwargs={'reasoning_content': ' complex calculations, use'} response_metadata={} id='run--1d220fee-cf9c-40a3-9bdc-a8bc8c868c64'\n",
      "content='' additional_kwargs={'reasoning_content': ' in AI, gaming'} response_metadata={} id='run--1d220fee-cf9c-40a3-9bdc-a8bc8c868c64'\n",
      "content='' additional_kwargs={'reasoning_content': ', graphics rendering.'} response_metadata={} id='run--1d220fee-cf9c-40a3-9bdc-a8bc8c868c64'\n",
      "content='' additional_kwargs={'reasoning_content': ' Maybe mention CUDA cores'} response_metadata={} id='run--1d220fee-cf9c-40a3-9bdc-a8bc8c868c64'\n",
      "content='' additional_kwargs={'reasoning_content': ' or something technical but'} response_metadata={} id='run--1d220fee-cf9c-40a3-9bdc-a8bc8c868c64'\n",
      "content='' additional_kwargs={'reasoning_content': ' keep it light.\\n\\n'} response_metadata={} id='run--1d220fee-cf9c-40a3-9bdc-a8bc8c868c64'\n",
      "content='' additional_kwargs={'reasoning_content': 'First line should set'} response_metadata={} id='run--1d220fee-cf9c-40a3-9bdc-a8bc8c868c64'\n",
      "content='' additional_kwargs={'reasoning_content': ' the scene. Maybe'} response_metadata={} id='run--1d220fee-cf9c-40a3-9bdc-a8bc8c868c64'\n",
      "content='' additional_kwargs={'reasoning_content': ' something like \"A'} response_metadata={} id='run--1d220fee-cf9c-40a3-9bdc-a8bc8c868c64'\n",
      "content='' additional_kwargs={'reasoning_content': \" GPU's might,\"} response_metadata={} id='run--1d220fee-cf9c-40a3-9bdc-a8bc8c868c64'\n",
      "content='' additional_kwargs={'reasoning_content': ' vast and unbounded'} response_metadata={} id='run--1d220fee-cf9c-40a3-9bdc-a8bc8c868c64'\n",
      "content='' additional_kwargs={'reasoning_content': ',\" to highlight power'} response_metadata={} id='run--1d220fee-cf9c-40a3-9bdc-a8bc8c868c64'\n",
      "content='' additional_kwargs={'reasoning_content': '. Then the second'} response_metadata={} id='run--1d220fee-cf9c-40a3-9bdc-a8bc8c868c64'\n",
      "content='' additional_kwargs={'reasoning_content': ' line could talk about'} response_metadata={} id='run--1d220fee-cf9c-40a3-9bdc-a8bc8c868c64'\n",
      "content='' additional_kwargs={'reasoning_content': ' what it does—'} response_metadata={} id='run--1d220fee-cf9c-40a3-9bdc-a8bc8c868c64'\n",
      "content='' additional_kwargs={'reasoning_content': 'speed through tasks.'} response_metadata={} id='run--1d220fee-cf9c-40a3-9bdc-a8bc8c868c64'\n",
      "content='' additional_kwargs={'reasoning_content': ' \"Speeds through'} response_metadata={} id='run--1d220fee-cf9c-40a3-9bdc-a8bc8c868c64'\n",
      "content='' additional_kwargs={'reasoning_content': ' tasks once deemed un'} response_metadata={} id='run--1d220fee-cf9c-40a3-9bdc-a8bc8c868c64'\n",
      "content='' additional_kwargs={'reasoning_content': 'manageable\" maybe'} response_metadata={} id='run--1d220fee-cf9c-40a3-9bdc-a8bc8c868c64'\n",
      "content='' additional_kwargs={'reasoning_content': '?\\n\\nThird and fourth'} response_metadata={} id='run--1d220fee-cf9c-40a3-9bdc-a8bc8c868c64'\n",
      "content='' additional_kwargs={'reasoning_content': ' lines need a different'} response_metadata={} id='run--1d220fee-cf9c-40a3-9bdc-a8bc8c868c64'\n",
      "content='' additional_kwargs={'reasoning_content': \" rhyme. Let's\"} response_metadata={} id='run--1d220fee-cf9c-40a3-9bdc-a8bc8c868c64'\n",
      "content='' additional_kwargs={'reasoning_content': ' think of specific uses'} response_metadata={} id='run--1d220fee-cf9c-40a3-9bdc-a8bc8c868c64'\n",
      "content='' additional_kwargs={'reasoning_content': ': AI training,'} response_metadata={} id='run--1d220fee-cf9c-40a3-9bdc-a8bc8c868c64'\n",
      "content='' additional_kwargs={'reasoning_content': ' rendering pixels. \"'} response_metadata={} id='run--1d220fee-cf9c-40a3-9bdc-a8bc8c868c64'\n",
      "content='' additional_kwargs={'reasoning_content': 'In realms of AI'} response_metadata={} id='run--1d220fee-cf9c-40a3-9bdc-a8bc8c868c64'\n",
      "content='' additional_kwargs={'reasoning_content': \" or pixels it's\"} response_metadata={} id='run--1d220fee-cf9c-40a3-9bdc-a8bc8c868c64'\n",
      "content='' additional_kwargs={'reasoning_content': ' rendering,\" then something'} response_metadata={} id='run--1d220fee-cf9c-40a3-9bdc-a8bc8c868c64'\n",
      "content='' additional_kwargs={'reasoning_content': ' about solving problems quickly'} response_metadata={} id='run--1d220fee-cf9c-40a3-9bdc-a8bc8c868c64'\n",
      "content='' additional_kwargs={'reasoning_content': '. \"Complex problems'} response_metadata={} id='run--1d220fee-cf9c-40a3-9bdc-a8bc8c868c64'\n",
      "content='' additional_kwargs={'reasoning_content': ' collapse, no pret'} response_metadata={} id='run--1d220fee-cf9c-40a3-9bdc-a8bc8c868c64'\n",
      "content='' additional_kwargs={'reasoning_content': 'ense needed.\"\\n\\nF'} response_metadata={} id='run--1d220fee-cf9c-40a3-9bdc-a8bc8c868c64'\n",
      "content='' additional_kwargs={'reasoning_content': 'ifth line needs to'} response_metadata={} id='run--1d220fee-cf9c-40a3-9bdc-a8bc8c868c64'\n",
      "content='' additional_kwargs={'reasoning_content': ' rhyme with the first'} response_metadata={} id='run--1d220fee-cf9c-40a3-9bdc-a8bc8c868c64'\n",
      "content='' additional_kwargs={'reasoning_content': ' two. Maybe something'} response_metadata={} id='run--1d220fee-cf9c-40a3-9bdc-a8bc8c868c64'\n",
      "content='' additional_kwargs={'reasoning_content': ' about the impact:'} response_metadata={} id='run--1d220fee-cf9c-40a3-9bdc-a8bc8c868c64'\n",
      "content='' additional_kwargs={'reasoning_content': ' \"The future’s'} response_metadata={} id='run--1d220fee-cf9c-40a3-9bdc-a8bc8c868c64'\n",
      "content='' additional_kwargs={'reasoning_content': ' bright where its power'} response_metadata={} id='run--1d220fee-cf9c-40a3-9bdc-a8bc8c868c64'\n",
      "content='' additional_kwargs={'reasoning_content': ' is harnessed'} response_metadata={} id='run--1d220fee-cf9c-40a3-9bdc-a8bc8c868c64'\n",
      "content='' additional_kwargs={'reasoning_content': '!\" That works.'} response_metadata={} id='run--1d220fee-cf9c-40a3-9bdc-a8bc8c868c64'\n",
      "content='' additional_kwargs={'reasoning_content': ' Let me check the'} response_metadata={} id='run--1d220fee-cf9c-40a3-9bdc-a8bc8c868c64'\n",
      "content='' additional_kwargs={'reasoning_content': ' syllable count.'} response_metadata={} id='run--1d220fee-cf9c-40a3-9bdc-a8bc8c868c64'\n",
      "content='' additional_kwargs={'reasoning_content': ' Limericks usually'} response_metadata={} id='run--1d220fee-cf9c-40a3-9bdc-a8bc8c868c64'\n",
      "content='' additional_kwargs={'reasoning_content': ' have a 8'} response_metadata={} id='run--1d220fee-cf9c-40a3-9bdc-a8bc8c868c64'\n",
      "content='' additional_kwargs={'reasoning_content': '-8-5'} response_metadata={} id='run--1d220fee-cf9c-40a3-9bdc-a8bc8c868c64'\n",
      "content='' additional_kwargs={'reasoning_content': '-5-8'} response_metadata={} id='run--1d220fee-cf9c-40a3-9bdc-a8bc8c868c64'\n",
      "content='' additional_kwargs={'reasoning_content': ' structure roughly. Let'} response_metadata={} id='run--1d220fee-cf9c-40a3-9bdc-a8bc8c868c64'\n",
      "content='' additional_kwargs={'reasoning_content': ' me count:\\n\\nA'} response_metadata={} id='run--1d220fee-cf9c-40a3-9bdc-a8bc8c868c64'\n",
      "content='' additional_kwargs={'reasoning_content': \" GPU's might,\"} response_metadata={} id='run--1d220fee-cf9c-40a3-9bdc-a8bc8c868c64'\n",
      "content='' additional_kwargs={'reasoning_content': ' vast and unbounded'} response_metadata={} id='run--1d220fee-cf9c-40a3-9bdc-a8bc8c868c64'\n",
      "content='' additional_kwargs={'reasoning_content': ', (8)\\n'} response_metadata={} id='run--1d220fee-cf9c-40a3-9bdc-a8bc8c868c64'\n",
      "content='' additional_kwargs={'reasoning_content': 'Speeds through tasks'} response_metadata={} id='run--1d220fee-cf9c-40a3-9bdc-a8bc8c868c64'\n",
      "content='' additional_kwargs={'reasoning_content': ' once deemed unmanage'} response_metadata={} id='run--1d220fee-cf9c-40a3-9bdc-a8bc8c868c64'\n",
      "content='' additional_kwargs={'reasoning_content': 'able. (9'} response_metadata={} id='run--1d220fee-cf9c-40a3-9bdc-a8bc8c868c64'\n",
      "content='' additional_kwargs={'reasoning_content': ')\\nIn realms of'} response_metadata={} id='run--1d220fee-cf9c-40a3-9bdc-a8bc8c868c64'\n",
      "content='' additional_kwargs={'reasoning_content': ' AI or pixels it'} response_metadata={} id='run--1d220fee-cf9c-40a3-9bdc-a8bc8c868c64'\n",
      "content='' additional_kwargs={'reasoning_content': \"'s rendering, (\"} response_metadata={} id='run--1d220fee-cf9c-40a3-9bdc-a8bc8c868c64'\n",
      "content='' additional_kwargs={'reasoning_content': '10)\\nComplex'} response_metadata={} id='run--1d220fee-cf9c-40a3-9bdc-a8bc8c868c64'\n",
      "content='' additional_kwargs={'reasoning_content': ' problems collapse, no'} response_metadata={} id='run--1d220fee-cf9c-40a3-9bdc-a8bc8c868c64'\n",
      "content='' additional_kwargs={'reasoning_content': ' pretense needed.'} response_metadata={} id='run--1d220fee-cf9c-40a3-9bdc-a8bc8c868c64'\n",
      "content='' additional_kwargs={'reasoning_content': ' (9)\\nThe'} response_metadata={} id='run--1d220fee-cf9c-40a3-9bdc-a8bc8c868c64'\n",
      "content='' additional_kwargs={'reasoning_content': ' future’s bright where'} response_metadata={} id='run--1d220fee-cf9c-40a3-9bdc-a8bc8c868c64'\n",
      "content='' additional_kwargs={'reasoning_content': ' its power is h'} response_metadata={} id='run--1d220fee-cf9c-40a3-9bdc-a8bc8c868c64'\n",
      "content='' additional_kwargs={'reasoning_content': 'arnessed! ('} response_metadata={} id='run--1d220fee-cf9c-40a3-9bdc-a8bc8c868c64'\n",
      "content='' additional_kwargs={'reasoning_content': '9)\\n\\nHmm,'} response_metadata={} id='run--1d220fee-cf9c-40a3-9bdc-a8bc8c868c64'\n",
      "content='' additional_kwargs={'reasoning_content': ' maybe trim some words'} response_metadata={} id='run--1d220fee-cf9c-40a3-9bdc-a8bc8c868c64'\n",
      "content='' additional_kwargs={'reasoning_content': \". Let's adjust\"} response_metadata={} id='run--1d220fee-cf9c-40a3-9bdc-a8bc8c868c64'\n",
      "content='' additional_kwargs={'reasoning_content': '. \"A GPU'} response_metadata={} id='run--1d220fee-cf9c-40a3-9bdc-a8bc8c868c64'\n",
      "content='' additional_kwargs={'reasoning_content': \"'s might, vast\"} response_metadata={} id='run--1d220fee-cf9c-40a3-9bdc-a8bc8c868c64'\n",
      "content='' additional_kwargs={'reasoning_content': ' and unbounded\"'} response_metadata={} id='run--1d220fee-cf9c-40a3-9bdc-a8bc8c868c64'\n",
      "content='' additional_kwargs={'reasoning_content': ' stays. Second line'} response_metadata={} id='run--1d220fee-cf9c-40a3-9bdc-a8bc8c868c64'\n",
      "content='' additional_kwargs={'reasoning_content': ': \"Speeds'} response_metadata={} id='run--1d220fee-cf9c-40a3-9bdc-a8bc8c868c64'\n",
      "content='' additional_kwargs={'reasoning_content': ' through tasks once quite'} response_metadata={} id='run--1d220fee-cf9c-40a3-9bdc-a8bc8c868c64'\n",
      "content='' additional_kwargs={'reasoning_content': ' unmanageable.\"'} response_metadata={} id='run--1d220fee-cf9c-40a3-9bdc-a8bc8c868c64'\n",
      "content='' additional_kwargs={'reasoning_content': \" That's 8\"} response_metadata={} id='run--1d220fee-cf9c-40a3-9bdc-a8bc8c868c64'\n",
      "content='' additional_kwargs={'reasoning_content': '. Third line:'} response_metadata={} id='run--1d220fee-cf9c-40a3-9bdc-a8bc8c868c64'\n",
      "content='' additional_kwargs={'reasoning_content': ' \"In AI realms'} response_metadata={} id='run--1d220fee-cf9c-40a3-9bdc-a8bc8c868c64'\n",
      "content='' additional_kwargs={'reasoning_content': ' or pixels rendering,\"'} response_metadata={} id='run--1d220fee-cf9c-40a3-9bdc-a8bc8c868c64'\n",
      "content='' additional_kwargs={'reasoning_content': ' Fourth: \"Pro'} response_metadata={} id='run--1d220fee-cf9c-40a3-9bdc-a8bc8c868c64'\n",
      "content='' additional_kwargs={'reasoning_content': 'blems collapse—no'} response_metadata={} id='run--1d220fee-cf9c-40a3-9bdc-a8bc8c868c64'\n",
      "content='' additional_kwargs={'reasoning_content': ' need for pretense'} response_metadata={} id='run--1d220fee-cf9c-40a3-9bdc-a8bc8c868c64'\n",
      "content='' additional_kwargs={'reasoning_content': '!\" Fifth: \"'} response_metadata={} id='run--1d220fee-cf9c-40a3-9bdc-a8bc8c868c64'\n",
      "content='' additional_kwargs={'reasoning_content': 'The future shines where'} response_metadata={} id='run--1d220fee-cf9c-40a3-9bdc-a8bc8c868c64'\n",
      "content='' additional_kwargs={'reasoning_content': \" its power's enhanced\"} response_metadata={} id='run--1d220fee-cf9c-40a3-9bdc-a8bc8c868c64'\n",
      "content='' additional_kwargs={'reasoning_content': '!\" Not perfect,'} response_metadata={} id='run--1d220fee-cf9c-40a3-9bdc-a8bc8c868c64'\n",
      "content='' additional_kwargs={'reasoning_content': ' but better. Alternatively'} response_metadata={} id='run--1d220fee-cf9c-40a3-9bdc-a8bc8c868c64'\n",
      "content='' additional_kwargs={'reasoning_content': ', \"The future'} response_metadata={} id='run--1d220fee-cf9c-40a3-9bdc-a8bc8c868c64'\n",
      "content='' additional_kwargs={'reasoning_content': '’s bright where its'} response_metadata={} id='run--1d220fee-cf9c-40a3-9bdc-a8bc8c868c64'\n",
      "content='' additional_kwargs={'reasoning_content': ' power is harn'} response_metadata={} id='run--1d220fee-cf9c-40a3-9bdc-a8bc8c868c64'\n",
      "content='' additional_kwargs={'reasoning_content': 'essed!\" might still'} response_metadata={} id='run--1d220fee-cf9c-40a3-9bdc-a8bc8c868c64'\n",
      "content='' additional_kwargs={'reasoning_content': ' work. Maybe tweak'} response_metadata={} id='run--1d220fee-cf9c-40a3-9bdc-a8bc8c868c64'\n",
      "content='' additional_kwargs={'reasoning_content': ' the last line.'} response_metadata={} id='run--1d220fee-cf9c-40a3-9bdc-a8bc8c868c64'\n",
      "content='' additional_kwargs={'reasoning_content': ' Also, check flow'} response_metadata={} id='run--1d220fee-cf9c-40a3-9bdc-a8bc8c868c64'\n",
      "content='' additional_kwargs={'reasoning_content': ' and meter. It'} response_metadata={} id='run--1d220fee-cf9c-40a3-9bdc-a8bc8c868c64'\n",
      "content='' additional_kwargs={'reasoning_content': ' should have a b'} response_metadata={} id='run--1d220fee-cf9c-40a3-9bdc-a8bc8c868c64'\n",
      "content='' additional_kwargs={'reasoning_content': 'ouncy rhythm. Maybe'} response_metadata={} id='run--1d220fee-cf9c-40a3-9bdc-a8bc8c868c64'\n",
      "content='' additional_kwargs={'reasoning_content': ' \"A GPU\\'s'} response_metadata={} id='run--1d220fee-cf9c-40a3-9bdc-a8bc8c868c64'\n",
      "content='' additional_kwargs={'reasoning_content': ' might, vast and'} response_metadata={} id='run--1d220fee-cf9c-40a3-9bdc-a8bc8c868c64'\n",
      "content='' additional_kwargs={'reasoning_content': ' unbounded, Speed'} response_metadata={} id='run--1d220fee-cf9c-40a3-9bdc-a8bc8c868c64'\n",
      "content='' additional_kwargs={'reasoning_content': 's through tasks once'} response_metadata={} id='run--1d220fee-cf9c-40a3-9bdc-a8bc8c868c64'\n",
      "content='' additional_kwargs={'reasoning_content': ' deemed unmanageable'} response_metadata={} id='run--1d220fee-cf9c-40a3-9bdc-a8bc8c868c64'\n",
      "content='' additional_kwargs={'reasoning_content': '. In realms of'} response_metadata={} id='run--1d220fee-cf9c-40a3-9bdc-a8bc8c868c64'\n",
      "content='' additional_kwargs={'reasoning_content': ' AI, pixels rendering'} response_metadata={} id='run--1d220fee-cf9c-40a3-9bdc-a8bc8c868c64'\n",
      "content='' additional_kwargs={'reasoning_content': ', Complex problems collapse'} response_metadata={} id='run--1d220fee-cf9c-40a3-9bdc-a8bc8c868c64'\n",
      "content='' additional_kwargs={'reasoning_content': ', no pretense'} response_metadata={} id='run--1d220fee-cf9c-40a3-9bdc-a8bc8c868c64'\n",
      "content='' additional_kwargs={'reasoning_content': ' needed. The future'} response_metadata={} id='run--1d220fee-cf9c-40a3-9bdc-a8bc8c868c64'\n",
      "content='' additional_kwargs={'reasoning_content': '’s bright where its'} response_metadata={} id='run--1d220fee-cf9c-40a3-9bdc-a8bc8c868c64'\n",
      "content='' additional_kwargs={'reasoning_content': ' power is harn'} response_metadata={} id='run--1d220fee-cf9c-40a3-9bdc-a8bc8c868c64'\n",
      "content='' additional_kwargs={'reasoning_content': 'essed!\" That seems'} response_metadata={} id='run--1d220fee-cf9c-40a3-9bdc-a8bc8c868c64'\n",
      "content='' additional_kwargs={'reasoning_content': ' okay. Let me'} response_metadata={} id='run--1d220fee-cf9c-40a3-9bdc-a8bc8c868c64'\n",
      "content='' additional_kwargs={'reasoning_content': \" see if there's\"} response_metadata={} id='run--1d220fee-cf9c-40a3-9bdc-a8bc8c868c64'\n",
      "content='' additional_kwargs={'reasoning_content': ' a more playful way'} response_metadata={} id='run--1d220fee-cf9c-40a3-9bdc-a8bc8c868c64'\n",
      "content='' additional_kwargs={'reasoning_content': '. Maybe use \"'} response_metadata={} id='run--1d220fee-cf9c-40a3-9bdc-a8bc8c868c64'\n",
      "content='' additional_kwargs={'reasoning_content': 'crunching\"'} response_metadata={} id='run--1d220fee-cf9c-40a3-9bdc-a8bc8c868c64'\n",
      "content='' additional_kwargs={'reasoning_content': ' instead of \"speed'} response_metadata={} id='run--1d220fee-cf9c-40a3-9bdc-a8bc8c868c64'\n",
      "content='' additional_kwargs={'reasoning_content': 's through\"? Or'} response_metadata={} id='run--1d220fee-cf9c-40a3-9bdc-a8bc8c868c64'\n",
      "content='' additional_kwargs={'reasoning_content': ' \"parallel cores\"'} response_metadata={} id='run--1d220fee-cf9c-40a3-9bdc-a8bc8c868c64'\n",
      "content='' additional_kwargs={'reasoning_content': ' to mention the architecture'} response_metadata={} id='run--1d220fee-cf9c-40a3-9bdc-a8bc8c868c64'\n",
      "content='' additional_kwargs={'reasoning_content': '. Hmm. Also'} response_metadata={} id='run--1d220fee-cf9c-40a3-9bdc-a8bc8c868c64'\n",
      "content='' additional_kwargs={'reasoning_content': ', ensure that the'} response_metadata={} id='run--1d220fee-cf9c-40a3-9bdc-a8bc8c868c64'\n",
      "content='' additional_kwargs={'reasoning_content': \" technical terms aren't\"} response_metadata={} id='run--1d220fee-cf9c-40a3-9bdc-a8bc8c868c64'\n",
      "content='' additional_kwargs={'reasoning_content': ' too obscure but recognizable'} response_metadata={} id='run--1d220fee-cf9c-40a3-9bdc-a8bc8c868c64'\n",
      "content='' additional_kwargs={'reasoning_content': '. CUDA could be'} response_metadata={} id='run--1d220fee-cf9c-40a3-9bdc-a8bc8c868c64'\n",
      "content='' additional_kwargs={'reasoning_content': ' a name drop but'} response_metadata={} id='run--1d220fee-cf9c-40a3-9bdc-a8bc8c868c64'\n",
      "content='' additional_kwargs={'reasoning_content': ' might not fit.'} response_metadata={} id='run--1d220fee-cf9c-40a3-9bdc-a8bc8c868c64'\n",
      "content='' additional_kwargs={'reasoning_content': ' Alternatively, \"parallel'} response_metadata={} id='run--1d220fee-cf9c-40a3-9bdc-a8bc8c868c64'\n",
      "content='' additional_kwargs={'reasoning_content': ' threads\" or \"'} response_metadata={} id='run--1d220fee-cf9c-40a3-9bdc-a8bc8c868c64'\n",
      "content='' additional_kwargs={'reasoning_content': 'streams of data\".'} response_metadata={} id='run--1d220fee-cf9c-40a3-9bdc-a8bc8c868c64'\n",
      "content='' additional_kwargs={'reasoning_content': ' Maybe not necessary.'} response_metadata={} id='run--1d220fee-cf9c-40a3-9bdc-a8bc8c868c64'\n",
      "content='' additional_kwargs={'reasoning_content': ' The key is to'} response_metadata={} id='run--1d220fee-cf9c-40a3-9bdc-a8bc8c868c64'\n",
      "content='' additional_kwargs={'reasoning_content': ' capture the essence without'} response_metadata={} id='run--1d220fee-cf9c-40a3-9bdc-a8bc8c868c64'\n",
      "content='' additional_kwargs={'reasoning_content': ' getting too technical.'} response_metadata={} id='run--1d220fee-cf9c-40a3-9bdc-a8bc8c868c64'\n",
      "content='' additional_kwargs={'reasoning_content': ' The limerick'} response_metadata={} id='run--1d220fee-cf9c-40a3-9bdc-a8bc8c868c64'\n",
      "content='' additional_kwargs={'reasoning_content': ' should be accessible.'} response_metadata={} id='run--1d220fee-cf9c-40a3-9bdc-a8bc8c868c64'\n",
      "content='' additional_kwargs={'reasoning_content': ' I think the initial'} response_metadata={} id='run--1d220fee-cf9c-40a3-9bdc-a8bc8c868c64'\n",
      "content='' additional_kwargs={'reasoning_content': ' draft works. Let'} response_metadata={} id='run--1d220fee-cf9c-40a3-9bdc-a8bc8c868c64'\n",
      "content='' additional_kwargs={'reasoning_content': ' me check another version'} response_metadata={} id='run--1d220fee-cf9c-40a3-9bdc-a8bc8c868c64'\n",
      "content='' additional_kwargs={'reasoning_content': '. Maybe start with'} response_metadata={} id='run--1d220fee-cf9c-40a3-9bdc-a8bc8c868c64'\n",
      "content='' additional_kwargs={'reasoning_content': ' \"There once was'} response_metadata={} id='run--1d220fee-cf9c-40a3-9bdc-a8bc8c868c64'\n",
      "content='' additional_kwargs={'reasoning_content': ' a GPU so grand'} response_metadata={} id='run--1d220fee-cf9c-40a3-9bdc-a8bc8c868c64'\n",
      "content='' additional_kwargs={'reasoning_content': ',\" typical limer'} response_metadata={} id='run--1d220fee-cf9c-40a3-9bdc-a8bc8c868c64'\n",
      "content='' additional_kwargs={'reasoning_content': 'ick opener. \"'} response_metadata={} id='run--1d220fee-cf9c-40a3-9bdc-a8bc8c868c64'\n",
      "content='' additional_kwargs={'reasoning_content': 'There once was a'} response_metadata={} id='run--1d220fee-cf9c-40a3-9bdc-a8bc8c868c64'\n",
      "content='' additional_kwargs={'reasoning_content': ' GPU so grand,'} response_metadata={} id='run--1d220fee-cf9c-40a3-9bdc-a8bc8c868c64'\n",
      "content='' additional_kwargs={'reasoning_content': ' That could process with'} response_metadata={} id='run--1d220fee-cf9c-40a3-9bdc-a8bc8c868c64'\n",
      "content='' additional_kwargs={'reasoning_content': ' lightning hand. For'} response_metadata={} id='run--1d220fee-cf9c-40a3-9bdc-a8bc8c868c64'\n",
      "content='' additional_kwargs={'reasoning_content': ' AI or games,'} response_metadata={} id='run--1d220fee-cf9c-40a3-9bdc-a8bc8c868c64'\n",
      "content='' additional_kwargs={'reasoning_content': ' It crunches the'} response_metadata={} id='run--1d220fee-cf9c-40a3-9bdc-a8bc8c868c64'\n",
      "content='' additional_kwargs={'reasoning_content': ' frames, And makes'} response_metadata={} id='run--1d220fee-cf9c-40a3-9bdc-a8bc8c868c64'\n",
      "content='' additional_kwargs={'reasoning_content': ' complex tasks seem quite'} response_metadata={} id='run--1d220fee-cf9c-40a3-9bdc-a8bc8c868c64'\n",
      "content='' additional_kwargs={'reasoning_content': ' bland!\" Hmm,'} response_metadata={} id='run--1d220fee-cf9c-40a3-9bdc-a8bc8c868c64'\n",
      "content='' additional_kwargs={'reasoning_content': \" that's another approach\"} response_metadata={} id='run--1d220fee-cf9c-40a3-9bdc-a8bc8c868c64'\n",
      "content='' additional_kwargs={'reasoning_content': '. But the user'} response_metadata={} id='run--1d220fee-cf9c-40a3-9bdc-a8bc8c868c64'\n",
      "content='' additional_kwargs={'reasoning_content': ' asked for \"w'} response_metadata={} id='run--1d220fee-cf9c-40a3-9bdc-a8bc8c868c64'\n",
      "content='' additional_kwargs={'reasoning_content': 'onders\", so maybe'} response_metadata={} id='run--1d220fee-cf9c-40a3-9bdc-a8bc8c868c64'\n",
      "content='' additional_kwargs={'reasoning_content': ' the first approach is'} response_metadata={} id='run--1d220fee-cf9c-40a3-9bdc-a8bc8c868c64'\n",
      "content='' additional_kwargs={'reasoning_content': \" better. I'll\"} response_metadata={} id='run--1d220fee-cf9c-40a3-9bdc-a8bc8c868c64'\n",
      "content='' additional_kwargs={'reasoning_content': ' go back. Maybe'} response_metadata={} id='run--1d220fee-cf9c-40a3-9bdc-a8bc8c868c64'\n",
      "content='' additional_kwargs={'reasoning_content': ' combine elements. \"'} response_metadata={} id='run--1d220fee-cf9c-40a3-9bdc-a8bc8c868c64'\n",
      "content='' additional_kwargs={'reasoning_content': \"A GPU's might\"} response_metadata={} id='run--1d220fee-cf9c-40a3-9bdc-a8bc8c868c64'\n",
      "content='' additional_kwargs={'reasoning_content': ', vast and un'} response_metadata={} id='run--1d220fee-cf9c-40a3-9bdc-a8bc8c868c64'\n",
      "content='' additional_kwargs={'reasoning_content': 'bounded, Speeds'} response_metadata={} id='run--1d220fee-cf9c-40a3-9bdc-a8bc8c868c64'\n",
      "content='' additional_kwargs={'reasoning_content': ' through tasks once deemed'} response_metadata={} id='run--1d220fee-cf9c-40a3-9bdc-a8bc8c868c64'\n",
      "content='' additional_kwargs={'reasoning_content': ' unmanageable.'} response_metadata={} id='run--1d220fee-cf9c-40a3-9bdc-a8bc8c868c64'\n",
      "content='' additional_kwargs={'reasoning_content': ' With cores working in'} response_metadata={} id='run--1d220fee-cf9c-40a3-9bdc-a8bc8c868c64'\n",
      "content='' additional_kwargs={'reasoning_content': ' tandem, It solves'} response_metadata={} id='run--1d220fee-cf9c-40a3-9bdc-a8bc8c868c64'\n",
      "content='' additional_kwargs={'reasoning_content': ' every demand, In'} response_metadata={} id='run--1d220fee-cf9c-40a3-9bdc-a8bc8c868c64'\n",
      "content='' additional_kwargs={'reasoning_content': ' realms of both gaming'} response_metadata={} id='run--1d220fee-cf9c-40a3-9bdc-a8bc8c868c64'\n",
      "content='' additional_kwargs={'reasoning_content': ' and manageable.\" No'} response_metadata={} id='run--1d220fee-cf9c-40a3-9bdc-a8bc8c868c64'\n",
      "content='' additional_kwargs={'reasoning_content': ', \"manageable'} response_metadata={} id='run--1d220fee-cf9c-40a3-9bdc-a8bc8c868c64'\n",
      "content='' additional_kwargs={'reasoning_content': '\" repeats. Maybe'} response_metadata={} id='run--1d220fee-cf9c-40a3-9bdc-a8bc8c868c64'\n",
      "content='' additional_kwargs={'reasoning_content': ' \"In realms of'} response_metadata={} id='run--1d220fee-cf9c-40a3-9bdc-a8bc8c868c64'\n",
      "content='' additional_kwargs={'reasoning_content': \" AI, it's\"} response_metadata={} id='run--1d220fee-cf9c-40a3-9bdc-a8bc8c868c64'\n",
      "content='' additional_kwargs={'reasoning_content': ' transcendent!\" Not'} response_metadata={} id='run--1d220fee-cf9c-40a3-9bdc-a8bc8c868c64'\n",
      "content='' additional_kwargs={'reasoning_content': ' sure. The original'} response_metadata={} id='run--1d220fee-cf9c-40a3-9bdc-a8bc8c868c64'\n",
      "content='' additional_kwargs={'reasoning_content': ' attempt might be better'} response_metadata={} id='run--1d220fee-cf9c-40a3-9bdc-a8bc8c868c64'\n",
      "content='' additional_kwargs={'reasoning_content': '. Let me stick'} response_metadata={} id='run--1d220fee-cf9c-40a3-9bdc-a8bc8c868c64'\n",
      "content='' additional_kwargs={'reasoning_content': ' with that and adjust'} response_metadata={} id='run--1d220fee-cf9c-40a3-9bdc-a8bc8c868c64'\n",
      "content='' additional_kwargs={'reasoning_content': ' as needed.'} response_metadata={} id='run--1d220fee-cf9c-40a3-9bdc-a8bc8c868c64'\n",
      "content='**A GPU' additional_kwargs={} response_metadata={} id='run--1d220fee-cf9c-40a3-9bdc-a8bc8c868c64'\n",
      "**A GPUcontent=\"'s might, vast\" additional_kwargs={} response_metadata={} id='run--1d220fee-cf9c-40a3-9bdc-a8bc8c868c64'\n",
      "'s might, vastcontent=' and unbounded,**' additional_kwargs={} response_metadata={} id='run--1d220fee-cf9c-40a3-9bdc-a8bc8c868c64'\n",
      " and unbounded,**content='  \\n**Speeds' additional_kwargs={} response_metadata={} id='run--1d220fee-cf9c-40a3-9bdc-a8bc8c868c64'\n",
      "  \n",
      "**Speedscontent=' through tasks once deemed' additional_kwargs={} response_metadata={} id='run--1d220fee-cf9c-40a3-9bdc-a8bc8c868c64'\n",
      " through tasks once deemedcontent=' unmanageable.' additional_kwargs={} response_metadata={} id='run--1d220fee-cf9c-40a3-9bdc-a8bc8c868c64'\n",
      " unmanageable.content='**  \\n**In' additional_kwargs={} response_metadata={} id='run--1d220fee-cf9c-40a3-9bdc-a8bc8c868c64'\n",
      "**  \n",
      "**Incontent=' realms of AI,' additional_kwargs={} response_metadata={} id='run--1d220fee-cf9c-40a3-9bdc-a8bc8c868c64'\n",
      " realms of AI,content=' pixels rendering,**  \\n' additional_kwargs={} response_metadata={} id='run--1d220fee-cf9c-40a3-9bdc-a8bc8c868c64'\n",
      " pixels rendering,**  \n",
      "content='**Complex problems collapse' additional_kwargs={} response_metadata={} id='run--1d220fee-cf9c-40a3-9bdc-a8bc8c868c64'\n",
      "**Complex problems collapsecontent='—no pretense' additional_kwargs={} response_metadata={} id='run--1d220fee-cf9c-40a3-9bdc-a8bc8c868c64'\n",
      "—no pretensecontent=' needed.**  \\n' additional_kwargs={} response_metadata={} id='run--1d220fee-cf9c-40a3-9bdc-a8bc8c868c64'\n",
      " needed.**  \n",
      "content='**The future’s' additional_kwargs={} response_metadata={} id='run--1d220fee-cf9c-40a3-9bdc-a8bc8c868c64'\n",
      "**The future’scontent=' bright where its power' additional_kwargs={} response_metadata={} id='run--1d220fee-cf9c-40a3-9bdc-a8bc8c868c64'\n",
      " bright where its powercontent='’s harnessed' additional_kwargs={} response_metadata={} id='run--1d220fee-cf9c-40a3-9bdc-a8bc8c868c64'\n",
      "’s harnessedcontent=', undeniable!**' additional_kwargs={} response_metadata={} id='run--1d220fee-cf9c-40a3-9bdc-a8bc8c868c64'\n",
      ", undeniable!**content='  \\n\\n*(This l' additional_kwargs={} response_metadata={} id='run--1d220fee-cf9c-40a3-9bdc-a8bc8c868c64'\n",
      "  \n",
      "\n",
      "*(This lcontent='imerick celebrates GPUs' additional_kwargs={} response_metadata={} id='run--1d220fee-cf9c-40a3-9bdc-a8bc8c868c64'\n",
      "imerick celebrates GPUscontent=\"' prowess in parallel\" additional_kwargs={} response_metadata={} id='run--1d220fee-cf9c-40a3-9bdc-a8bc8c868c64'\n",
      "' prowess in parallelcontent=' processing, AI,' additional_kwargs={} response_metadata={} id='run--1d220fee-cf9c-40a3-9bdc-a8bc8c868c64'\n",
      " processing, AI,content=' and graphics, while' additional_kwargs={} response_metadata={} id='run--1d220fee-cf9c-40a3-9bdc-a8bc8c868c64'\n",
      " and graphics, whilecontent=' sticking to the classic' additional_kwargs={} response_metadata={} id='run--1d220fee-cf9c-40a3-9bdc-a8bc8c868c64'\n",
      " sticking to the classiccontent=' rhyme and rhythm.' additional_kwargs={} response_metadata={} id='run--1d220fee-cf9c-40a3-9bdc-a8bc8c868c64'\n",
      " rhyme and rhythm.content=' \"CUDA cores\"' additional_kwargs={} response_metadata={} id='run--1d220fee-cf9c-40a3-9bdc-a8bc8c868c64'\n",
      " \"CUDA cores\"content=' or \"tensor threads' additional_kwargs={} response_metadata={} id='run--1d220fee-cf9c-40a3-9bdc-a8bc8c868c64'\n",
      " or \"tensor threadscontent='\" could take a' additional_kwargs={} response_metadata={} id='run--1d220fee-cf9c-40a3-9bdc-a8bc8c868c64'\n",
      "\" could take acontent=' bow, but sometimes' additional_kwargs={} response_metadata={} id='run--1d220fee-cf9c-40a3-9bdc-a8bc8c868c64'\n",
      " bow, but sometimescontent=' wonder wears a simpler' additional_kwargs={} response_metadata={} id='run--1d220fee-cf9c-40a3-9bdc-a8bc8c868c64'\n",
      " wonder wears a simplercontent=' face.)*' additional_kwargs={} response_metadata={} id='run--1d220fee-cf9c-40a3-9bdc-a8bc8c868c64'\n",
      " face.)*content='' additional_kwargs={} response_metadata={'finish_reason': 'stop', 'model_name': 'qwen3-235b-a22b'} id='run--1d220fee-cf9c-40a3-9bdc-a8bc8c868c64'\n"
     ]
    }
   ],
   "source": [
    "# Standard library imports\n",
    "from typing import Any, Dict, Iterator, List, Mapping, Optional, Type, Union, cast\n",
    "\n",
    "# Third-party imports\n",
    "import openai\n",
    "from langchain_core.callbacks import CallbackManagerForLLMRun\n",
    "from langchain_core.messages import (\n",
    "    AIMessageChunk,\n",
    "    BaseMessage,\n",
    "    BaseMessageChunk,\n",
    "    ChatMessageChunk,\n",
    "    FunctionMessageChunk,\n",
    "    HumanMessageChunk,\n",
    "    SystemMessageChunk,\n",
    "    ToolMessageChunk,\n",
    ")\n",
    "from langchain_core.messages.ai import UsageMetadata\n",
    "from langchain_core.messages.tool import tool_call_chunk\n",
    "from langchain_core.outputs import ChatGenerationChunk, ChatResult\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_openai.chat_models.base import (\n",
    "    _create_usage_metadata,\n",
    "    _handle_openai_bad_request,\n",
    "    warnings,\n",
    ")\n",
    "\n",
    "def _convert_delta_to_message_chunk(\n",
    "    delta_dict: Mapping[str, Any], default_class: Type[BaseMessageChunk]\n",
    ") -> BaseMessageChunk:\n",
    "    \"\"\"Convert a delta dictionary to a message chunk.\n",
    "    \n",
    "    Args:\n",
    "        delta_dict: Dictionary containing delta information from OpenAI response\n",
    "        default_class: Default message chunk class to use if role is not specified\n",
    "        \n",
    "    Returns:\n",
    "        BaseMessageChunk: Appropriate message chunk based on role and content\n",
    "        \n",
    "    Raises:\n",
    "        KeyError: If required keys are missing from the delta dictionary\n",
    "    \"\"\"\n",
    "    message_id = delta_dict.get(\"id\")\n",
    "    role = cast(str, delta_dict.get(\"role\", \"\"))\n",
    "    content = cast(str, delta_dict.get(\"content\") or \"\")\n",
    "    additional_kwargs: Dict[str, Any] = {}\n",
    "    \n",
    "    # Handle function calls\n",
    "    if function_call_data := delta_dict.get(\"function_call\"):\n",
    "        function_call = dict(function_call_data)\n",
    "        if \"name\" in function_call and function_call[\"name\"] is None:\n",
    "            function_call[\"name\"] = \"\"\n",
    "        additional_kwargs[\"function_call\"] = function_call\n",
    "    \n",
    "    # Handle tool calls\n",
    "    tool_call_chunks = []\n",
    "    if raw_tool_calls := delta_dict.get(\"tool_calls\"):\n",
    "        additional_kwargs[\"tool_calls\"] = raw_tool_calls\n",
    "        try:\n",
    "            tool_call_chunks = [\n",
    "                tool_call_chunk(\n",
    "                    name=rtc.get(\"function\", {}).get(\"name\"),\n",
    "                    args=rtc.get(\"function\", {}).get(\"arguments\"),\n",
    "                    id=rtc.get(\"id\"),\n",
    "                    index=rtc.get(\"index\", 0),\n",
    "                )\n",
    "                for rtc in raw_tool_calls\n",
    "                if rtc.get(\"function\")  # Ensure function key exists\n",
    "            ]\n",
    "        except (KeyError, TypeError) as e:\n",
    "            # Log the error but continue processing\n",
    "            pass\n",
    "\n",
    "    # Return appropriate message chunk based on role\n",
    "    if role == \"user\" or default_class == HumanMessageChunk:\n",
    "        return HumanMessageChunk(content=content, id=message_id)\n",
    "    elif role == \"assistant\" or default_class == AIMessageChunk:\n",
    "        # Handle reasoning content for OpenAI reasoning models\n",
    "        if reasoning_content := delta_dict.get(\"reasoning_content\"):\n",
    "            additional_kwargs[\"reasoning_content\"] = reasoning_content\n",
    "        return AIMessageChunk(\n",
    "            content=content,\n",
    "            additional_kwargs=additional_kwargs,\n",
    "            id=message_id,\n",
    "            tool_call_chunks=tool_call_chunks,  # type: ignore[arg-type]\n",
    "        )\n",
    "    elif role in (\"system\", \"developer\") or default_class == SystemMessageChunk:\n",
    "        if role == \"developer\":\n",
    "            additional_kwargs = {\"__openai_role__\": \"developer\"}\n",
    "        return SystemMessageChunk(\n",
    "            content=content, id=message_id, additional_kwargs=additional_kwargs\n",
    "        )\n",
    "    elif role == \"function\" or default_class == FunctionMessageChunk:\n",
    "        function_name = delta_dict.get(\"name\", \"\")\n",
    "        return FunctionMessageChunk(content=content, name=function_name, id=message_id)\n",
    "    elif role == \"tool\" or default_class == ToolMessageChunk:\n",
    "        tool_call_id = delta_dict.get(\"tool_call_id\", \"\")\n",
    "        return ToolMessageChunk(\n",
    "            content=content, tool_call_id=tool_call_id, id=message_id\n",
    "        )\n",
    "    elif role or default_class == ChatMessageChunk:\n",
    "        return ChatMessageChunk(content=content, role=role, id=message_id)\n",
    "    else:\n",
    "        return default_class(content=content, id=message_id)  # type: ignore\n",
    "\n",
    "def _convert_chunk_to_generation_chunk(\n",
    "    chunk: Dict[str, Any], \n",
    "    default_chunk_class: Type[BaseMessageChunk], \n",
    "    base_generation_info: Optional[Dict[str, Any]]\n",
    ") -> Optional[ChatGenerationChunk]:\n",
    "    \"\"\"Convert a streaming chunk to a generation chunk.\n",
    "    \n",
    "    Args:\n",
    "        chunk: Raw chunk data from OpenAI streaming response\n",
    "        default_chunk_class: Default message chunk class to use\n",
    "        base_generation_info: Base generation information to include\n",
    "        \n",
    "    Returns:\n",
    "        Optional[ChatGenerationChunk]: Generated chunk or None if chunk should be skipped\n",
    "    \"\"\"\n",
    "    # Skip content.delta type chunks from beta.chat.completions.stream\n",
    "    if chunk.get(\"type\") == \"content.delta\":\n",
    "        return None\n",
    "    \n",
    "    token_usage = chunk.get(\"usage\")\n",
    "    choices = (\n",
    "        chunk.get(\"choices\", [])\n",
    "        # Handle chunks from beta.chat.completions.stream format\n",
    "        or chunk.get(\"chunk\", {}).get(\"choices\", [])\n",
    "    )\n",
    "\n",
    "    usage_metadata: Optional[UsageMetadata] = (\n",
    "        _create_usage_metadata(token_usage) if token_usage else None\n",
    "    )\n",
    "    \n",
    "    # Handle empty choices\n",
    "    if not choices:\n",
    "        generation_chunk = ChatGenerationChunk(\n",
    "            message=default_chunk_class(content=\"\", usage_metadata=usage_metadata)\n",
    "        )\n",
    "        return generation_chunk\n",
    "\n",
    "    choice = choices[0]\n",
    "    if choice.get(\"delta\") is None:\n",
    "        return None\n",
    "\n",
    "    message_chunk = _convert_delta_to_message_chunk(\n",
    "        choice[\"delta\"], default_chunk_class\n",
    "    )\n",
    "    generation_info = dict(base_generation_info) if base_generation_info else {}\n",
    "\n",
    "    # Add finish reason and model info if available\n",
    "    if finish_reason := choice.get(\"finish_reason\"):\n",
    "        generation_info[\"finish_reason\"] = finish_reason\n",
    "        if model_name := chunk.get(\"model\"):\n",
    "            generation_info[\"model_name\"] = model_name\n",
    "        if system_fingerprint := chunk.get(\"system_fingerprint\"):\n",
    "            generation_info[\"system_fingerprint\"] = system_fingerprint\n",
    "\n",
    "    # Add log probabilities if available\n",
    "    if logprobs := choice.get(\"logprobs\"):\n",
    "        generation_info[\"logprobs\"] = logprobs\n",
    "\n",
    "    # Attach usage metadata to AI message chunks\n",
    "    if usage_metadata and isinstance(message_chunk, AIMessageChunk):\n",
    "        message_chunk.usage_metadata = usage_metadata\n",
    "\n",
    "    generation_chunk = ChatGenerationChunk(\n",
    "        message=message_chunk, generation_info=generation_info or None\n",
    "    )\n",
    "    return generation_chunk\n",
    "\n",
    "class ChatOpenAIReasoning(ChatOpenAI):\n",
    "    \"\"\"Extended ChatOpenAI model with reasoning capabilities.\n",
    "    \n",
    "    This class extends the base ChatOpenAI model to support OpenAI's reasoning models\n",
    "    that include reasoning_content in their responses. It handles the extraction and\n",
    "    preservation of reasoning content during both streaming and non-streaming operations.\n",
    "    \"\"\"\n",
    "\n",
    "    def _create_chat_result(\n",
    "        self,\n",
    "        response: Union[Dict[str, Any], openai.BaseModel],\n",
    "        generation_info: Optional[Dict[str, Any]] = None,\n",
    "    ) -> ChatResult:\n",
    "        \"\"\"Create a chat result from the OpenAI response.\n",
    "        \n",
    "        Args:\n",
    "            response: The response from OpenAI API\n",
    "            generation_info: Additional generation information\n",
    "            \n",
    "        Returns:\n",
    "            ChatResult: The formatted chat result with reasoning content if available\n",
    "        \"\"\"\n",
    "        chat_result = super()._create_chat_result(response, generation_info)\n",
    "\n",
    "        # Only process BaseModel responses (not raw dict responses)\n",
    "        if not isinstance(response, openai.BaseModel):\n",
    "            return chat_result\n",
    "\n",
    "        # Extract reasoning content if available\n",
    "        try:\n",
    "            if (hasattr(response, 'choices') and \n",
    "                response.choices and \n",
    "                hasattr(response.choices[0], 'message') and\n",
    "                hasattr(response.choices[0].message, \"reasoning_content\")):\n",
    "                \n",
    "                reasoning_content = response.choices[0].message.reasoning_content\n",
    "                if reasoning_content and chat_result.generations:\n",
    "                    chat_result.generations[0].message.additional_kwargs[\"reasoning_content\"] = reasoning_content\n",
    "        except (IndexError, AttributeError):\n",
    "            # If reasoning content extraction fails, continue without it\n",
    "            pass\n",
    "\n",
    "        return chat_result\n",
    "    \n",
    "    def _stream(\n",
    "        self,\n",
    "        messages: List[BaseMessage],\n",
    "        stop: Optional[List[str]] = None,\n",
    "        run_manager: Optional[CallbackManagerForLLMRun] = None,\n",
    "        **kwargs: Any,\n",
    "    ) -> Iterator[ChatGenerationChunk]:\n",
    "        \"\"\"Create a streaming generator for chat completions.\n",
    "        \n",
    "        Args:\n",
    "            messages: List of messages to send to the model\n",
    "            stop: Optional list of stop sequences\n",
    "            run_manager: Optional callback manager for LLM runs\n",
    "            **kwargs: Additional keyword arguments for the API call\n",
    "            \n",
    "        Yields:\n",
    "            ChatGenerationChunk: Individual chunks from the streaming response\n",
    "            \n",
    "        Raises:\n",
    "            openai.BadRequestError: If the API request is invalid\n",
    "        \"\"\"\n",
    "        kwargs[\"stream\"] = True\n",
    "        payload = self._get_request_payload(messages, stop=stop, **kwargs)\n",
    "        default_chunk_class: Type[BaseMessageChunk] = AIMessageChunk\n",
    "        base_generation_info: Dict[str, Any] = {}\n",
    "\n",
    "        # Handle response format for beta completions\n",
    "        if \"response_format\" in payload:\n",
    "            if self.include_response_headers:\n",
    "                warnings.warn(\n",
    "                    \"Cannot currently include response headers when response_format is \"\n",
    "                    \"specified.\"\n",
    "                )\n",
    "            payload.pop(\"stream\")\n",
    "            response_stream = self.root_client.beta.chat.completions.stream(**payload)\n",
    "            context_manager = response_stream\n",
    "        else:\n",
    "            # Handle regular streaming with optional response headers\n",
    "            if self.include_response_headers:\n",
    "                raw_response = self.client.with_raw_response.create(**payload)\n",
    "                response = raw_response.parse()\n",
    "                base_generation_info = {\"headers\": dict(raw_response.headers)}\n",
    "            else:\n",
    "                response = self.client.create(**payload)\n",
    "            context_manager = response\n",
    "\n",
    "        try:\n",
    "            with context_manager as response:\n",
    "                is_first_chunk = True\n",
    "                for chunk in response:\n",
    "                    # Convert chunk to dict if it's a model object\n",
    "                    if not isinstance(chunk, dict):\n",
    "                        chunk = chunk.model_dump()\n",
    "                    \n",
    "                    generation_chunk = _convert_chunk_to_generation_chunk(\n",
    "                        chunk,\n",
    "                        default_chunk_class,\n",
    "                        base_generation_info if is_first_chunk else {},\n",
    "                    )\n",
    "                    \n",
    "                    if generation_chunk is None:\n",
    "                        continue\n",
    "                    \n",
    "                    # Update default chunk class for subsequent chunks\n",
    "                    default_chunk_class = generation_chunk.message.__class__\n",
    "                    \n",
    "                    # Handle log probabilities for callback\n",
    "                    logprobs = (generation_chunk.generation_info or {}).get(\"logprobs\")\n",
    "                    if run_manager:\n",
    "                        run_manager.on_llm_new_token(\n",
    "                            generation_chunk.text,\n",
    "                            chunk=generation_chunk,\n",
    "                            logprobs=logprobs,\n",
    "                        )\n",
    "                    \n",
    "                    is_first_chunk = False\n",
    "                    yield generation_chunk\n",
    "                    \n",
    "        except openai.BadRequestError as e:\n",
    "            _handle_openai_bad_request(e)\n",
    "        \n",
    "        # Handle final completion for response_format requests\n",
    "        if (hasattr(response, \"get_final_completion\") and \n",
    "            \"response_format\" in payload):\n",
    "            try:\n",
    "                final_completion = response.get_final_completion()\n",
    "                generation_chunk = self._get_generation_chunk_from_completion(\n",
    "                    final_completion\n",
    "                )\n",
    "                if run_manager:\n",
    "                    run_manager.on_llm_new_token(\n",
    "                        generation_chunk.text, chunk=generation_chunk\n",
    "                    )\n",
    "                yield generation_chunk\n",
    "            except AttributeError:\n",
    "                # If get_final_completion method doesn't exist, continue without it\n",
    "                pass\n",
    "client = ChatOpenAIReasoning(\n",
    "  #base_url=\"https://integrate.api.nvidia.com/v1\",\n",
    "  #model=\"qwen/qwen3-235b-a22b\",\n",
    "  #api_key=\"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\", \n",
    "  base_url=\"https://dashscope.aliyuncs.com/compatible-mode/v1\",\n",
    "  model=\"qwen3-235b-a22b\",\n",
    "  api_key=\"sk-ba9202fabcb249e291c5907a86b13f80\",\n",
    "  temperature=0.6,\n",
    "  top_p=0.7,\n",
    "  max_tokens=8192,\n",
    "  #extra_body={\"chat_template_kwargs\": {\"thinking\":True}},\n",
    "  extra_body={\"enable_thinking\": True},\n",
    "  stream_usage=True, \n",
    ")\n",
    " \n",
    "response = client.stream(\n",
    "  input=[{\"role\":\"system\",\"content\":\"\"},\n",
    "   {\"role\":\"user\",\"content\":\"Write a limerick about the wonders of GPU computing.\"}\n",
    "   ])\n",
    "for chunk in response:\n",
    "    print(chunk)\n",
    "    print(chunk.text(), end=\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deer-flow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
